{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b12c41-a5df-488d-a4c0-5bfa611ac876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "#import wandb\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer, AdamW, AutoModelForCausalLM, BitsAndBytesConfig, EarlyStoppingCallback, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" \n",
    "#torch.backends.cuda.matmul.allow_tf32=True\n",
    "#torch.set_float32_matmul_precision('medium')\n",
    "#torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0fdd3e-a3fb-41d7-9630-32227808604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "cfg = {\n",
    "    'runtime' : datetime.now(timezone('Asia/Seoul')).strftime(\"%y%m%d%H%M\"), # cfg를 생성한 시각 -> model dir의 마지막 부분에 들어감\n",
    "    'model_name' : 'LDCC-SOLAR-10.7B',\n",
    "    'trainer_name' : 'QLoRA + 4bit quantization + LDCC-SOLAR-10.7B',\n",
    "    'random_seed' : 2024, # seed\n",
    "\n",
    "    # train_config \n",
    "    'train_strategy' : 'epoch', # 'epoch' or 'steps' \n",
    "    'train_size' : 0.9, # train size\n",
    "    'test_size' : 0.1, # evaluation size\n",
    "    'shuffle' : True, # shuffle when train_test_split\n",
    "    'train_batch_size' : 32, # train batch_size / max_length : 512 기준 128, 64 -> CUDA OOM  -> 32 !!\n",
    "    'eval_batch_size' : 8, # evaluation batch_size / train_batch_size 낮춰주면서 같이 낮춰줌\n",
    "    'lr' : 1e-4, # learning rate\n",
    "    'optimizer' : \"paged_adamw_8bit\", # optimizer\n",
    "\n",
    "    # steps config\n",
    "    'max_steps' : 50,  # use only when train_strategy is 'steps'\n",
    "    'logging_steps' : 5, # use only when train_strategy is 'steps'\n",
    "    'save_steps' : 5, # default:500 / use only when train_strategy is 'steps' \n",
    "\n",
    "    # epoch config\n",
    "    'num_train_epochs' : 5, # use only when train_stratey is 'epoch'\n",
    "    \n",
    "    \n",
    "    # evaluation(validation) config\n",
    "    'eval_strategy' : 'epoch', # how often do eval -> unify with 'train_strategy'\n",
    "    'do_eval' : True, # evaluate or not \n",
    "\n",
    "    # save config - logs and models\n",
    "    'save_strategy' : 'epoch', # how often save a model -> unify with 'train_strategy'\n",
    "    'log_dir' : './logs', # log directory\n",
    "    'log_strategy' : 'epoch', # how often write log -> unify with 'train_strategy'\n",
    "    'model_dir' : './models/', # model directory to save trained model \n",
    "    'save_total_limit' : 3, # number of model saved (include best model)\n",
    "    'load_best_model_at_end' : True, # load a best model at the end of training and also save\n",
    "    'metric_for_best_model' : 'loss', # metric for select the best model : loss -> eval_loss\n",
    "    'greater_is_better' : False, # greater metric(loss) is better?\n",
    "\n",
    "    # device config\n",
    "    'device_map' : 'balanced', # gpu mapping\n",
    "\n",
    "    # token config\n",
    "    'max_length' : 512, # max length of encoding by tokenizer / 20240307 기존 128에서 512로 변경 (학습 데이터가 잘리는 것을 목격)\n",
    "    'max_token' : 512, # max number of tokens generated by model(inference) : \n",
    "    'padding_side' : 'left', # location of padding \n",
    "    'padding' : 'max_length', # use tokenizer.encode(padding='max_length', args)\n",
    "    'return_tensor' : 'pt', # tokenizer return tensor 'pt' : pytorch, 'tf' : tensorflow\n",
    "}\n",
    "\n",
    "# Trainer \n",
    "training_argument = TrainingArguments(\n",
    "    do_eval=cfg['do_eval'],\n",
    "    per_device_train_batch_size=cfg['train_batch_size'], #  128\n",
    "    per_device_eval_batch_size=cfg['eval_batch_size'], # 16\n",
    "    evaluation_strategy = cfg['train_strategy'], \n",
    "    # gradient_accumulation_steps=1, # 큰 배치 사이즈를 사용하지 못하는 경우 이전 스텝의 gradient를 축적하여 사용하는 것\n",
    "    # max_steps=cfg['max_steps'], # default=1\n",
    "    num_train_epochs=cfg['num_train_epochs'], # 10 , when select 'stpes' at train_strategy set as comment this line\n",
    "    learning_rate=cfg['lr'], # 1e-4\n",
    "    fp16=True,\n",
    "    # logging_steps=cfg['logging_steps'], # how often write log\n",
    "    # save_steps=cfg['save_steps'], # how often save \n",
    "    logging_dir = cfg['log_dir'],# + cfg['trainer_name'] + '-' + cfg['runtime'], # 학습 및 평가 로그 저장 디렉토리 -> 제대로 로그 저장이 안됨, 그냥 로그가 저장이 안되는 것 같다.\n",
    "    logging_strategy=cfg['train_strategy'],\n",
    "    output_dir = cfg['model_dir'] + cfg['trainer_name']+ '-' + cfg['runtime'],\n",
    "    optim= cfg['optimizer'], # paged_adamw_8bit\n",
    "    save_total_limit=cfg['save_total_limit'],\n",
    "    load_best_model_at_end = cfg['load_best_model_at_end'], \n",
    "    save_strategy = cfg['train_strategy'],\n",
    "    metric_for_best_model=cfg['metric_for_best_model'],\n",
    "    greater_is_better = cfg['greater_is_better'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854bf839-d1e2-47c8-b621-e9856ff91085",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45120736-5dfd-4196-894c-a7ea44f88d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 \n",
    "data = pd.read_csv('./data/train_data_final.csv')\n",
    "\n",
    "# 토크나이저 로드\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained('LDCC/LDCC-SOLAR-10.7B',\n",
    "#                                                     eos_token='</s>',\n",
    "#                                                     #pad_token= -100,\n",
    "#                                                     device_map=cfg['device_map'],\n",
    "#                                                     # padding='right', # left is better \n",
    "#                                                    )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('LDCC/LDCC-SOLAR-10.7B',\n",
    "                                          eos_token='</s>',\n",
    "                                          device_map=cfg['device_map'],\n",
    "                                          padding_side=cfg['padding_side'],\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7a6af9-64cd-41a7-a7ef-683dc9e0419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6440it [00:02, 2662.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 포맷팅 및 토큰화\n",
    "formatted_data = []\n",
    "for _, row in tqdm(data.iterrows()):\n",
    "    input_text = row['질문'] + tokenizer.eos_token + row['답변']\n",
    "    input_enc = tokenizer.encode(input_text, return_tensors=cfg['return_tensor'],\n",
    "                                 padding=cfg['padding'],\n",
    "                                 truncation=cfg['shuffle'],\n",
    "                                 max_length=cfg['max_length']) # every tokenized data has 128 length\n",
    "    formatted_data.append(input_enc)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db7b729-8acb-4999-a272-62a06830ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "train_data, valid_data = train_test_split(formatted_data, \n",
    "                                          train_size=cfg['train_size'],\n",
    "                                          test_size=cfg['test_size'],\n",
    "                                          shuffle=cfg['shuffle'], \n",
    "                                          random_state=cfg['random_seed']\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe4c9ee-9661-461a-aa46-1413529d4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatted_data = torch.cat(formatted_data, dim=0)\n",
    "formatted_train_data = torch.cat(train_data, dim=0)\n",
    "formatted_valid_data = torch.cat(valid_data, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1784fd-acec-4916-aa1f-56ea908a2e31",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b2db0d-f48c-4431-8e9a-92691478ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fac6a8e41bd4b6081f97d9c9bc71bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 31457280 || all params: 10894053376 || trainable%: 0.28875643357229686\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "\n",
    "model_id = \"LDCC/LDCC-SOLAR-10.7B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "#                                              quantization_config=bnb_config,\n",
    "#                                              device_map=cfg['device_map'] \n",
    "#                                              # torch_dtype=torch.float32,\n",
    "#                                              )\n",
    "\n",
    "# load best model before\n",
    "peft_model_id = './models/QLoRA + 4bit quantization + LDCC-SOLAR-10.7B-2403062219/checkpoint-414-best'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                            device_map=cfg['device_map'],\n",
    "                                            )\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    #target_modules=[\"query_key_value\"], \n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987f2252-e4bb-41ac-8ac0-85dc0cf07b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trainer 정의\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_train_data,\n",
    "    eval_dataset=formatted_valid_data,\n",
    "    args=training_argument,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da0f66-647b-47b4-9a17-77b5e59e40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train!\n",
      "Start time : 2024-03-07 19:33:36.198487+09:00\n",
      "train epochs : 5\n",
      "logging dir : ./logs\n",
      "output_dir : ./models/QLoRA + 4bit quantization + LDCC-SOLAR-10.7B-2403071930\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='365' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [365/910 2:19:01 < 3:28:43, 0.04 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.588605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/81 00:17 < 02:40, 0.45 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Start train!')\n",
    "print(f'Start time : {datetime.now(timezone(\"Asia/Seoul\"))}')\n",
    "print(f'train epochs : {training_argument.num_train_epochs}') # training_argument.train_steps 로 변경 필요\n",
    "# print(f'logging steps : {training_argument.logging_steps}')\n",
    "print(f'logging dir : {training_argument.logging_dir}')\n",
    "print(f'output_dir : {training_argument.output_dir}')\n",
    "trainer.train()\n",
    "print('End train!')\n",
    "print(f'End time : {datetime.now(timezone(\"Asia/Seoul\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c92d4b-a7fd-4e98-890b-000019516759",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7b8b3-a702-4086-9210-82f52b19c6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4498e-bccb-4e31-b968-885f93dfb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained('LDCC/LDCC-SOLAR-10.7B',\n",
    "#                                                     eos_token='</s>',\n",
    "#                                                    device_map='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82cd6e-627a-4cea-9b98-de8859fc25fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e66e3-18bc-4ae1-adb9-fa4954712a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "preds = []\n",
    "\n",
    "for test_q in tqdm(test['질문']):\n",
    "    # 입력 텍스트를 토큰화 하고 모델 입력 형태로 변환\n",
    "    input_ = tokenizer(test_q + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    outputs = model.generate(input_.input_ids.to(model.device), \n",
    "                             max_new_tokens=cfg['max_token'], \n",
    "                            )\n",
    "\n",
    "    # 생성된 텍스트(답변) 저장\n",
    "    full_text = tokenizer.batch_decode(outputs.detach().cpu(), skip_special_tokens=False)\n",
    "    # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
    "    full_text = full_text[0]\n",
    "    answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)\n",
    "    answer_only = full_text[answer_start:].strip()\n",
    "    answer_only = answer_only.replace('\\n', ' ')\n",
    "    preds.append(answer_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c1c3f-f428-46f2-844f-0e674c27f123",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f58b1-ac29-48cc-b756-c5af9148461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "modelEmb = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "pred_embeddings = modelEmb.encode(preds)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00a5c6-0f9d-4900-8dc8-8a5b09162712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
    "submit.iloc[:,1:] = pred_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0450bf-6959-4902-965c-702da4feb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리더보드 제출을 위한 csv파일 생성\n",
    "\n",
    "submit.to_csv(f'./submissions/' + cfg['trainer_name']+ '-' + cfg['runtime']+'512tokens'+'.csv', \n",
    "              index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
